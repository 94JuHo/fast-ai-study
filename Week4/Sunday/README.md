# List
| | Part | Page | Speaker 1 | Speaker 2 |
|:-:|:-----:|:----:|:---------:|:---------:|
|1|[Pixels: The Foundations of Computer Vision](#1-1)|133| | |
|1|[First Try: Pixel Similarity:NumPy Arrays and PyTorch Tensors](#1-2)|145| | |
|2|[Computing Metrics Using Broadcastingn](#2-1)|145| | |
|2|[Stochastic Gradient Descent:Calculating Gradients](#2-2)|156| | |
|3|[Stochastic Gradient Descent:Stepping with a Learning Rate](#3-1)|157| | |
|3|[Stochastic Gradient Descent:Summarizing Gradient Descent](#3-2)|163| | |
|4|[The MNIST Loss Function:Sigmoid](#4-1)|163| | |
|4|[The MNIST Loss Function:SGD and Mini-Batches](#4-2)|171| | |
|5|[Putting It All Together:Creating an Optimizer](#5-1)|171| | |
|5|[Questionnaire:Further Research](#5-2)|184| | |



---

<div id="1-1"></div>
<div id="1-2"></div>

### 1-1. Pixels: The Foundations of Computer Vision
### 1-2. First Try: Pixel Similarity:NumPy Arrays and PyTorch Tensors
* 발표자료 : [ ]()

    

<div id="2-1"></div>
<div id="2-2"></div>
    
### 2-1. Computing Metrics Using Broadcasting
### 2-2. Stochastic Gradient Descent:Calculating Gradients
* 발표자료 : [ ]()
    


<div id="3-1"></div>
<div id="3-2"></div>

### 3-1. Stochastic Gradient Descent:Stepping with a Learning Rate
### 3-2. Stochastic Gradient Descent:Summarizing Gradient Descent
* 발표자료 : [ ]()
    




<div id="4-1"></div>
<div id="4-2"></div>

### 4-1. The MNIST Loss Function:Sigmoid
### 4-2. The MNIST Loss Function:SGD and Mini-Batches
* 발표자료 : [ ]()
    






### 5-1. Putting It All Together:Creating an Optimizer
### 5-2. Questionnaire:Further Research
* 발표자료 : [ ]()
  