# List
| | Chapter | Page | Speaker 1 | Speaker 2 |
|:-:|:-----:|:----:|:---------:|:---------:|
|1|[Pixels: The Foundations of Computer Vision](#1-1)|133| | |
|1|[First Try: Pixel Similarity:NumPy Arrays and PyTorch Tensors](#1-2)|145| | |
|2|[Computing Metrics Using Broadcastingn](#2-1)|145| | |
|2|[Stochastic Gradient Descent:Calculating Gradients](#2-2)|156| | |
|3|[Stochastic Gradient Descent:Stepping with a Learning Rate](#3-1)|157| | |
|3|[Stochastic Gradient Descent:Summarizing Gradient Descent](#3-2)|163| | |
|4|[The MNIST Loss Function:Sigmoid](#4-1)|163| | |
|4|[The MNIST Loss Function:SGD and Mini-Batches](#4-2)|171| | |
|5|[Putting It All Together:Creating an Optimizer](#5-1)|171| | |
|5|[Questionnaire:Further Research](#5-2)|184| | |



---


### 1-1. Pixels: The Foundations of Computer Vision
### 1-2. First Try: Pixel Similarity:NumPy Arrays and PyTorch Tensors
[ ]()

    


    
### 2-1. Computing Metrics Using Broadcasting
### 2-2. Stochastic Gradient Descent:Calculating Gradients
[ ]()
    




### 3-1. Stochastic Gradient Descent:Stepping with a Learning Rate
### 3-2. Stochastic Gradient Descent:Summarizing Gradient Descent
[ ]()
    






### 4-1. The MNIST Loss Function:Sigmoid
### 4-2. The MNIST Loss Function:SGD and Mini-Batches
[ ]()
    






### 5-1. Putting It All Together:Creating an Optimizer
### 5-2. Questionnaire:Further Research
[ ]()
  