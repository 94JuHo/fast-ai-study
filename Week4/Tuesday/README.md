# List
| | Part | Page | Speaker 1 | Speaker 2 |
|:-:|:-----:|:----:|:---------:|:---------:|
|1|[Pixels: The Foundations of Computer Vision](#1-1)|133|김용현|송석리|
|1|[First Try: Pixel Similarity:NumPy Arrays and PyTorch Tensors](#1-2)|145|김용현|송석리|
|2|[Computing Metrics Using Broadcasting](#2-1)|145|이준택| |
|2|[Stochastic Gradient Descent:Calculating Gradients](#2-2)|156|이준택| |
|3|[Stochastic Gradient Descent:Stepping with a Learning Rate](#3-1)|157|김지은| |
|3|[Stochastic Gradient Descent:Summarizing Gradient Descent](#3-2)|163|김지은| |
|4|[The MNIST Loss Function:Sigmoid](#4-1)|163|강인승| |
|4|[The MNIST Loss Function:SGD and Mini-Batches](#4-2)|171|강인승| |
|5|[Putting It All Together:Creating an Optimizer](#5-1)|171|정경훈| |
|5|[Questionnaire:Further Research](#5-2)|184|정경훈| |



---

<div id="1-1"></div>
<div id="1-2"></div>

#### 1-1. Pixels: The Foundations of Computer Vision / 1-2. First Try: Pixel Similarity:NumPy Arrays and PyTorch Tensors
* 발표자료 : [김용현]() / [송석리](4th_Week_Tue_01_송석리_Neural%20Network%20Basic%20Model.pdf)

    

<div id="2-1"></div>
<div id="2-2"></div>
    
#### 2-1. Computing Metrics Using Broadcasting / 2-2. Stochastic Gradient Descent:Calculating Gradients
* 발표자료 : [이준택](4th_Week_Tue_02_이준택.pptx)
    


<div id="3-1"></div>
<div id="3-2"></div>

#### 3-1. Stochastic Gradient Descent:Stepping with a Learning Rate / 3-2. Stochastic Gradient Descent:Summarizing Gradient Descent
* 발표자료 : [김지은](4th_Week_Tue_03_김지은.pdf)
    




<div id="4-1"></div>
<div id="4-2"></div>

#### 4-1. The MNIST Loss Function:Sigmoid / 4-2. The MNIST Loss Function:SGD and Mini-Batches
* 발표자료 : [강인승](4th_Week_Tue_04_강인승.pdf)
    




<div id="5-1"></div>
<div id="5-2"></div>

#### 5-1. Putting It All Together:Creating an Optimizer / 5-2. Questionnaire:Further Research
* 발표자료 : [정경훈](4th_Week_Tue_05_정경훈.pdf)
  
