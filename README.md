# List
| | Part | Page |
|:-:|:-----:|:----:|
|1|[Deep Learning Is for Everyone](Week1)|3|
|1|[How to Learn Deep Learning](Week1)|12|
|2|[The Software: PyTorch, fastai, and Jupyter (And Why It Doesnâ€™t Matter)](Week1)|12|
|2|[Running Your First Notebook](Week1)|20|
|3|[What Is Machine Learning?](Week1)|20|
|3|[Limitations Inherent to ML](Week1)|26|
|4|[How Our Image Recognizer Works](Week1)|26|
|4|[What Our Image Recognizer Learned](Week1)|36|
|5|[Image Recognizers Can Tackle Non-Image Tasks](Week1)|36|
|5|[Deep Learning Is Not Just for Image Classification](Week1)|48|
|6|[Validation Sets and Test Sets](Week1)|48|
|6|[A Choose Your Own Adventure Moment](Week1)|54|
|7|[The Preactice of Deep Learning](Week2)|57|
|7|[The Practice of Deep Learning : The Drivetrain Approach](Week2)|65|
|8|[Gathering Data](Week2)|65|
|8|[Gathering Data](Week2)|70|
|9|[From Data to DataLoaders](Week2)|70|
|9|[Training Your Model, and Using It to Clean Your Data](Week2)|78|
|10|[Tuning Your Model into an Online Application](Week2)|78|
|10|[Tuning Your Model into an Online Application:Deploying Your App](Week2)|86|
|11|[How to Avoid Disaster](Week2)|86|
|11|[Questionaire : Further Research](Week2)|92|
|12|[Key Example for Data Ethnics](Week3)|93|
|12|[Key Example for Data Ethnics](Week3)|99|
|13|[Integrating Machine Learning with Product Design](Week3)|99|
|13|[Topics in Data Ethnics : Feedback Loops](Week3)|105|
|14|[Topics in Data Ethnics : Bias](Week3)|105|
|14|[Topics in Data Ethnics : Bias](Week3)|116|
|15|[Topics in Data Ethnics : Disinformation](Week3)|116|
|15|[Identifying and Addressing Ethical Issues:Fairness, Accountability, and Transparency](Week3)|123|
|16|[Role of Policy](Week3)|123|
|16|[Deep Learning in Practice : That's a Wrap!](Week3)|128|
|17|[Pixels: The Foundations of Computer Vision](Week4)|133|
|17|[First Try: Pixel Similarity:NumPy Arrays and PyTorch Tensors](Week4)|145|
|18|[Computing Metrics Using Broadcastingn](Week4)|145|
|18|[Stochastic Gradient Descent:Calculating Gradients](Week4)|156|
|19|[Stochastic Gradient Descent:Stepping with a Learning Rate](Week4)|157|
|19|[Stochastic Gradient Descent:Summarizing Gradient Descent](Week4)|163|
|20|[The MNIST Loss Function:Sigmoid](Week4)|163|
|20|[The MNIST Loss Function:SGD and Mini-Batches](Week4)|171|
|21|[Putting It All Together:Creating an Optimizer](Week4)|171|
|21|[Questionnaire:Further Research](Week4)|184|
|22|[From Dogs and Cats to Pet Breeds](Week5)|185|
|22|[Presizing:Checking and Debugging a DataBlocks](Week5)|194|
|23|[Cross-Entropy Loss](Week5)|194|
|23|[Cross-Entropy Loss:Taking the log](Week5)|203|
|24|[Model Interpretation](Week5)|203|
|24|[Improving Our Model:Unfreezing and Transfer Learning](Week5)|210|
|25|[Improving Our Model:Discriminative Learning Rates](Week5)|210|
|25|[Questionnaire:Further Research](Week5)|217|



---

